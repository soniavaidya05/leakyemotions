{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky Emotions Analysis Notebook\n",
    "# ===================================\n",
    "\n",
    "This notebook provides tools for analyzing TensorBoard results from state punishment experiments.\n",
    "It includes data processing, visualization, and statistical analysis functions.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Imports and Dependencies](#1-imports-and-dependencies)\n",
    "2. [TensorBoard Data Processing](#2-tensorboard-data-processing)\n",
    "3. [Data Analysis Utilities](#3-data-analysis-utilities)\n",
    "4. [Visualization Functions](#4-visualization-functions)\n",
    "5. [Statistical Analysis](#5-statistical-analysis)\n",
    "6. [Data Processing Pipeline](#6-data-processing-pipeline)\n",
    "7. [Example Analysis](#7-example-analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core data processing and visualization\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# TensorFlow for TensorBoard processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.errors_impl import DataLossError\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorBoard Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_to_csv(event_file, csv_file):\n",
    "    \"\"\"\n",
    "    Convert TensorBoard event file data to a CSV format.\n",
    "\n",
    "    Args:\n",
    "        event_file (str): Path to the TensorBoard event file (e.g., events.out.tfevents.xxx).\n",
    "        csv_file (str): Path where the output CSV file should be saved.\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "    \n",
    "    # Use tf.compat.v1 to access the summary_iterator in TensorFlow 2.x\n",
    "    for e in tf.compat.v1.train.summary_iterator(event_file):\n",
    "        for v in e.summary.value:\n",
    "            # Only consider scalar summaries\n",
    "            if v.HasField('simple_value'):\n",
    "                tag = v.tag\n",
    "                value = v.simple_value\n",
    "                step = e.step\n",
    "                data_rows.append([step, tag, value])\n",
    "    \n",
    "    # Write the extracted data into a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Step', 'Tag', 'Value'])\n",
    "        writer.writerows(data_rows)\n",
    "    \n",
    "    print(f\"Data from {event_file} has been written to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_to_separate_csv(event_file, output_dir):\n",
    "    \"\"\"\n",
    "    Convert TensorBoard event file to separate CSV files for each metric.\n",
    "    \n",
    "    Args:\n",
    "        event_file (str): Path to the TensorBoard event file\n",
    "        output_dir (str): Directory where CSV files should be saved\n",
    "    \"\"\"\n",
    "    tag_data = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        for e in tf.compat.v1.train.summary_iterator(event_file):\n",
    "            try:\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        tag = v.tag\n",
    "                        value = v.simple_value\n",
    "                        step = e.step\n",
    "                        tag_data[tag].append([step, value])\n",
    "            except Exception as record_error:\n",
    "                print(f\"Skipped a corrupt record in file: {event_file}\")\n",
    "    except DataLossError:\n",
    "        print(f\"Encountered DataLossError. Possibly due to incomplete writes in file: {event_file}\")\n",
    "\n",
    "    # Save tag data to CSV files\n",
    "    for tag, data_rows in tag_data.items():\n",
    "        filename = f\"{output_dir}/{tag.replace('/', '_')}_data.csv\"\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Step', 'Value'])\n",
    "            writer.writerows(data_rows)\n",
    "        print(f\"Data for tag '{tag}' has been written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_stable(file_path, wait_time=1.0):\n",
    "    \"\"\"Check if a file is stable (not being written to).\"\"\"\n",
    "    initial_size = os.path.getsize(file_path)\n",
    "    time.sleep(wait_time)\n",
    "    final_size = os.path.getsize(file_path)\n",
    "    return initial_size == final_size\n",
    "\n",
    "def process_tensorboard_results(parent_dir, output_parent_dir):\n",
    "    \"\"\"\n",
    "    Process all TensorBoard files in a directory structure.\n",
    "    \n",
    "    Args:\n",
    "        parent_dir (str): Parent directory containing TensorBoard event files\n",
    "        output_parent_dir (str): Parent directory where CSV files should be saved\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            if \"tfevents\" in file:\n",
    "                event_file = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, parent_dir)\n",
    "                output_dir = os.path.join(output_parent_dir, relative_path)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                try:\n",
    "                    tensorboard_to_separate_csv(event_file, output_dir)\n",
    "                    print(f\"Processed {event_file} -> {output_dir}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {event_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a rolling window correlation function\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def check_percentage_of_above_threshold(array, threshold, prop):\n",
    "    proportion = np.mean(array > threshold)\n",
    "    if proportion >= prop:\n",
    "        return 1\n",
    "    elif np.mean(array < threshold) >= prop:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def rolling_window_correlation(x, y, window):\n",
    "    \"\"\"\n",
    "    Calculate the rolling window correlation and p-value between two 1D arrays.\n",
    "\n",
    "    Args:\n",
    "        x (array-like): First time series.\n",
    "        y (array-like): Second time series.\n",
    "        window (int): Window size for rolling correlation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (corrs, pvals)\n",
    "            corrs: np.ndarray of correlation coefficients, length = len(x) - window + 1\n",
    "            pvals: np.ndarray of p-values, same length\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Input arrays must have the same length.\")\n",
    "    if window < 1 or window > len(x):\n",
    "        raise ValueError(\"Window size must be between 1 and the length of the input arrays.\")\n",
    "\n",
    "    corrs = []\n",
    "    pvals = []\n",
    "    prop =[]\n",
    "    for i in range(len(x) - window + 1):\n",
    "        x_win = x[i:i+window]\n",
    "        y_win = y[i:i+window]\n",
    "        # Remove nan pairs\n",
    "        mask = ~np.isnan(x_win) & ~np.isnan(y_win)\n",
    "        if np.sum(mask) < 2:\n",
    "            corrs.append(np.nan)\n",
    "            pvals.append(np.nan)\n",
    "            prop.append(np.nan)\n",
    "        else:\n",
    "            r, p = pearsonr(x_win[mask], y_win[mask])\n",
    "            corrs.append(r)\n",
    "            pvals.append(p)\n",
    "            prop.append(check_percentage_of_above_threshold(x_win[mask], 0.3, 0.6))\n",
    "    return np.array(corrs), np.array(pvals), np.array(prop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_and_calculate_mean(array_list):\n",
    "    \"\"\"\n",
    "    Trim arrays to the same length and calculate mean.\n",
    "    \n",
    "    Args:\n",
    "        array_list: List of arrays to process\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of trimmed arrays\n",
    "    \"\"\"\n",
    "    min_length = min(len(arr) for arr in array_list)\n",
    "    trimmed_arrays = [arr[:min_length] for arr in array_list]\n",
    "    return np.array(trimmed_arrays)\n",
    "\n",
    "def exponential_moving_average(data, alpha):\n",
    "    \"\"\"\n",
    "    Calculate the exponential moving average (EMA) of a 1D array.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data\n",
    "        alpha (float): The smoothing factor (0 < alpha <= 1)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The EMA values\n",
    "    \"\"\"\n",
    "    if not (0 < alpha <= 1):\n",
    "        raise ValueError(\"Alpha must be between 0 and 1.\")\n",
    "\n",
    "    ema = [data[0]]\n",
    "    for i in range(1, len(data)):\n",
    "        ema.append(alpha * data[i] + (1 - alpha) * ema[-1])\n",
    "    return np.array(ema)\n",
    "\n",
    "def rolling_average(data, window_size):\n",
    "    \"\"\"\n",
    "    Calculate the rolling average of a 1D array.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data\n",
    "        window_size (int): The size of the rolling window\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The rolling average values\n",
    "    \"\"\"\n",
    "    if window_size < 1:\n",
    "        raise ValueError(\"Window size must be at least 1.\")\n",
    "    if len(data) < window_size:\n",
    "        raise ValueError(\"Data length must be at least equal to the window size.\")\n",
    "    \n",
    "    weights = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, weights, mode='valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_trajectory(time_series, error_type='std', time_points=None, \n",
    "                           xlabel='Time', ylabel='Value', title='Average Trajectory'):\n",
    "    \"\"\"\n",
    "    Plot the average trajectory of a set of time series with error bars.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series (2D array-like): A set of time series, shape (n_series, n_time_points)\n",
    "    - error_type (str): Either 'std' for standard deviation or 'sem' for standard error\n",
    "    - time_points (1D array-like, optional): Time points corresponding to the time series\n",
    "    - xlabel (str): Label for the x-axis\n",
    "    - ylabel (str): Label for the y-axis\n",
    "    - title (str): Title of the plot\n",
    "    \"\"\"\n",
    "    time_series = np.array(time_series)\n",
    "    if time_points is None:\n",
    "        time_points = np.arange(time_series.shape[1])\n",
    "    else:\n",
    "        time_points = np.array(time_points)\n",
    "    \n",
    "    if time_series.shape[1] != len(time_points):\n",
    "        raise ValueError(\"Length of time_points must match the number of columns in time_series.\")\n",
    "    \n",
    "    # Compute average and error\n",
    "    mean_trajectory = np.mean(time_series, axis=0)\n",
    "    if error_type == 'std':\n",
    "        error = np.std(time_series, axis=0)\n",
    "    elif error_type == 'sem':\n",
    "        error = np.std(time_series, axis=0) / np.sqrt(time_series.shape[0])\n",
    "    else:\n",
    "        raise ValueError(\"error_type must be 'std' or 'sem'.\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_points, mean_trajectory, label='Mean Trajectory', color='blue')\n",
    "    plt.fill_between(time_points, mean_trajectory - error, mean_trajectory + error, \n",
    "                     alpha=0.3, color='blue', label='Error')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_multiple_conditions(data_list, labels, window_size=200, title=\"Comparison\"):\n",
    "    \"\"\"\n",
    "    Plot multiple conditions with rolling average smoothing.\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of data arrays for each condition\n",
    "        labels: List of labels for each condition\n",
    "        window_size: Window size for rolling average\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, (data, label) in enumerate(zip(data_list, labels)):\n",
    "        if len(data) > window_size:\n",
    "            smoothed = rolling_average(data, window_size)\n",
    "            plt.plot(smoothed, label=label, alpha=0.8)\n",
    "        else:\n",
    "            plt.plot(data, label=label, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_of_variation(timestamps):\n",
    "    \"\"\"\n",
    "    Calculate the coefficient of variation (CV) of inter-visit intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    - timestamps: A list or array of timestamps representing the times of visits or events\n",
    "    \n",
    "    Returns:\n",
    "    - CV: The coefficient of variation (standard deviation / mean) of inter-visit intervals\n",
    "    \"\"\"\n",
    "    inter_visit_intervals = np.diff(timestamps)\n",
    "    mean_interval = np.mean(inter_visit_intervals)\n",
    "    std_dev_interval = np.std(inter_visit_intervals)\n",
    "    cv = std_dev_interval / mean_interval\n",
    "    return cv\n",
    "\n",
    "def fourier_transform_periodicity(data):\n",
    "    \"\"\"\n",
    "    Perform a Fourier Transform to find the dominant frequency and quantify periodicity.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A list or array representing the time series data\n",
    "    \n",
    "    Returns:\n",
    "    - dominant_period: The period corresponding to the dominant frequency\n",
    "    - power_at_dominant_frequency: The strength of the dominant frequency\n",
    "    \"\"\"\n",
    "    fft_result = np.fft.fft(data)\n",
    "    frequencies = np.fft.fftfreq(len(data))\n",
    "    power = np.abs(fft_result)\n",
    "    \n",
    "    dominant_frequency = frequencies[np.argmax(power)]\n",
    "    dominant_period = 1 / dominant_frequency\n",
    "    \n",
    "    return dominant_period, power[np.argmax(power)]\n",
    "\n",
    "def correlation_analysis(x, y, method='spearman'):\n",
    "    \"\"\"\n",
    "    Perform correlation analysis between two variables.\n",
    "    \n",
    "    Args:\n",
    "        x, y: Arrays to correlate\n",
    "        method: 'pearson' or 'spearman'\n",
    "        \n",
    "    Returns:\n",
    "        correlation coefficient and p-value\n",
    "    \"\"\"\n",
    "    if method == 'pearson':\n",
    "        return pearsonr(x, y)\n",
    "    elif method == 'spearman':\n",
    "        return spearmanr(x, y)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pearson' or 'spearman'\")\n",
    "\n",
    "def multiple_regression_analysis(data_dict, target_var):\n",
    "    \"\"\"\n",
    "    Perform multiple regression analysis.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary with variable names as keys and arrays as values\n",
    "        target_var: Name of the target variable\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with regression results\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_vars = [col for col in df.columns if col != target_var]\n",
    "    X = df[feature_vars]\n",
    "    y = df[target_var]\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'intercept': model.intercept_,\n",
    "        'coefficients': dict(zip(feature_vars, model.coef_)),\n",
    "        'mse': mse,\n",
    "        'r2': r2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(folders, entity_names, agent_filter=None):\n",
    "    \"\"\"\n",
    "    Load and process data from multiple experiment folders.\n",
    "    \n",
    "    Args:\n",
    "        folders: List of experiment folder names\n",
    "        entity_names: List of entity/metric names to load\n",
    "        agent_filter: Optional filter for specific agents (e.g., 'Agent_0')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processed data for each folder and entity\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for folder in folders:\n",
    "        results[folder] = {}\n",
    "        parent_dir = os.path.join('res', folder)\n",
    "        \n",
    "        if not os.path.exists(parent_dir):\n",
    "            print(f\"Warning: Directory {parent_dir} does not exist\")\n",
    "            continue\n",
    "            \n",
    "        files = os.listdir(parent_dir)\n",
    "        \n",
    "        for entity_name in entity_names:\n",
    "            entity_data = []\n",
    "            \n",
    "            for f in files:\n",
    "                if entity_name in f:\n",
    "                    if agent_filter is None or agent_filter in f:\n",
    "                        file_path = os.path.join(parent_dir, f)\n",
    "                        try:\n",
    "                            data = pd.read_csv(file_path)\n",
    "                            entity_data.append(data['Value'].to_numpy())\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reading {file_path}: {e}\")\n",
    "            \n",
    "            if entity_data:\n",
    "                # Calculate mean across runs\n",
    "                processed_data = np.mean(trim_and_calculate_mean(entity_data), axis=0)\n",
    "                results[folder][entity_name] = processed_data\n",
    "            else:\n",
    "                print(f\"No data found for {entity_name} in {folder}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_conditions(data_dict, metric_name, labels=None, window_size=200):\n",
    "    \"\"\"\n",
    "    Compare a specific metric across different conditions.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary with experiment data\n",
    "        metric_name: Name of the metric to compare\n",
    "        labels: Optional labels for conditions\n",
    "        window_size: Window size for smoothing\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    condition_labels = []\n",
    "    \n",
    "    for folder, metrics in data_dict.items():\n",
    "        if metric_name in metrics:\n",
    "            data_list.append(metrics[metric_name])\n",
    "            condition_labels.append(folder if labels is None else labels[len(data_list)-1])\n",
    "    \n",
    "    if data_list:\n",
    "        plot_multiple_conditions(data_list, condition_labels, window_size, f\"{metric_name} Comparison\")\n",
    "    else:\n",
    "        print(f\"No data found for metric: {metric_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process TensorBoard results\n",
    "# Uncomment and modify paths as needed\n",
    "\n",
    "parent_dir = \n",
    "output_dir = \n",
    "process_tensorboard_results(parent_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and analyze experiment data\n",
    "\n",
    "# Define experiment folders\n",
    "example_folders = [\n",
    "    'experiment_1',\n",
    "    'experiment_2', \n",
    "    'experiment_3'\n",
    "]\n",
    "\n",
    "# Define metrics to analyze\n",
    "metrics = ['Reward', 'Loss', 'Epsilon']\n",
    "\n",
    "# Load data\n",
    "# experiment_data = load_experiment_data(example_folders, metrics)\n",
    "\n",
    "# Compare rewards across conditions\n",
    "# compare_conditions(experiment_data, 'Reward', \n",
    "#                   labels=['Condition 1', 'Condition 2', 'Condition 3'])\n",
    "\n",
    "print(\"Example analysis functions ready. Uncomment and modify as needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Statistical analysis\n",
    "\n",
    "# Generate example data for demonstration\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(1000)\n",
    "y = 2 * x + np.random.randn(1000) * 0.5\n",
    "\n",
    "# Correlation analysis\n",
    "corr_coef, p_value = correlation_analysis(x, y, method='pearson')\n",
    "print(f\"Pearson correlation: {corr_coef:.3f}, p-value: {p_value:.3e}\")\n",
    "\n",
    "# Multiple regression example\n",
    "example_data = {\n",
    "    'X1': x,\n",
    "    'X2': np.random.randn(1000),\n",
    "    'Y': y\n",
    "}\n",
    "\n",
    "regression_results = multiple_regression_analysis(example_data, 'Y')\n",
    "print(f\"\\nRegression Results:\")\n",
    "print(f\"RÂ²: {regression_results['r2']:.3f}\")\n",
    "print(f\"MSE: {regression_results['mse']:.3f}\")\n",
    "print(f\"Coefficients: {regression_results['coefficients']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sorrel)",
   "language": "python",
   "name": "sorrel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
