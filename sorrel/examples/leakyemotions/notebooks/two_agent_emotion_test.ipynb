{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath('../../../../'))\n",
    "\n",
    "# sorrel imports\n",
    "from sorrel.examples.leakyemotions.agents import LeakyEmotionsAgent, Wolf\n",
    "from sorrel.examples.leakyemotions.entities import EmptyEntity\n",
    "from sorrel.examples.leakyemotions.env import LeakyEmotionsEnv, ENTITY_LIST\n",
    "from sorrel.examples.leakyemotions.world import LeakyEmotionsWorld\n",
    "from sorrel.examples.leakyemotions.wolf_model import WolfModel\n",
    "from sorrel.action.action_spec import ActionSpec\n",
    "from sorrel.models.human_player import HumanPlayer, HumanObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for spawn_prob in [0.001, 0.002, 0.003, 0.004]:\n",
    "        config = OmegaConf.create({\n",
    "            \"experiment\": {\n",
    "                \"epochs\": 1,\n",
    "                \"max_turns\": 50,\n",
    "                \"record_period\": 50,\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"agent_vision_radius\": 3,\n",
    "                \"epsilon_decay\": 0.0001,\n",
    "            },\n",
    "            \"world\": {\n",
    "                \"agents\": 2,\n",
    "                \"height\": 9,\n",
    "                \"width\": 9,\n",
    "                \"layers\": 3,\n",
    "                \"spawn_prob\": spawn_prob,\n",
    "                \"wolves\":0,\n",
    "            },\n",
    "        })\n",
    "\n",
    "    world = LeakyEmotionsWorld(config=config, default_entity=EmptyEntity())\n",
    "    env = LeakyEmotionsEnv(world, config)\n",
    "    agents = env.agents\n",
    "\n",
    "    observation_spec = HumanObservation(\n",
    "        entity_list=ENTITY_LIST,\n",
    "        full_view=True,\n",
    "        env_dims=(config.world.height, config.world.width),\n",
    "    )\n",
    "    action_spec = ActionSpec([\"up\", \"down\", \"left\", \"right\"])\n",
    "    agents[-2] = LeakyEmotionsAgent(\n",
    "        observation_spec=observation_spec,\n",
    "        action_spec=action_spec,\n",
    "        model=HumanPlayer(\n",
    "            input_size=(config.world.height, config.world.width, config.world.layers),\n",
    "            action_space=action_spec.n_actions,\n",
    "            memory_size=1,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    env.override_agents(agents=agents)\n",
    "    env.run_experiment()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HumanPlayer' object has no attribute 'state_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m experiment = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m agents[-\u001b[32m2\u001b[39m] = LeakyEmotionsAgent(\n\u001b[32m     33\u001b[39m     observation_spec=observation_spec,\n\u001b[32m     34\u001b[39m     action_spec=action_spec,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     ),\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m env.override_agents(agents=agents)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/leakyemotions/sorrel/examples/leakyemotions/env.py:229\u001b[39m, in \u001b[36mLeakyEmotionsEnv.run_experiment\u001b[39m\u001b[34m(self, animate, logging, logger, output_dir)\u001b[39m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m animate_this_turn \u001b[38;5;129;01mand\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    228\u001b[39m         renderer.add_image(\u001b[38;5;28mself\u001b[39m.world)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m     bunnies_left = \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28misinstance\u001b[39m(agent, LeakyEmotionsAgent) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents]) - \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.world.dead_agents)\n\u001b[32m    232\u001b[39m \u001b[38;5;28mself\u001b[39m.world.is_done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/leakyemotions/sorrel/environment.py:86\u001b[39m, in \u001b[36mEnvironment.take_turn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m         x.transition(\u001b[38;5;28mself\u001b[39m.world)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/leakyemotions/sorrel/agents/agent.py:136\u001b[39m, in \u001b[36mAgent.transition\u001b[39m\u001b[34m(self, world)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Processes a full transition step for the agent.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m \u001b[33;03mThis function does the following:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m    env (Gridworld): the environment that this agent is acting in.\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    135\u001b[39m state = \u001b[38;5;28mself\u001b[39m.pov(world)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m reward = \u001b[38;5;28mself\u001b[39m.act(world, action)\n\u001b[32m    138\u001b[39m done = \u001b[38;5;28mself\u001b[39m.is_done(world)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/leakyemotions/sorrel/examples/leakyemotions/agents.py:49\u001b[39m, in \u001b[36mLeakyEmotionsAgent.get_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     47\u001b[39m     model_input = stacked_states.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Update the agent emotion.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_emotion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     action = \u001b[38;5;28mself\u001b[39m.model.take_action(model_input)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/leakyemotions/sorrel/examples/leakyemotions/agents.py:63\u001b[39m, in \u001b[36mLeakyEmotionsAgent.update_emotion\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_emotion\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: np.ndarray) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the agent's emotion based on its state value approximation.\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m        state: The observed input.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28mself\u001b[39m.emotion = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_value\u001b[49m(state)\n",
      "\u001b[31mAttributeError\u001b[39m: 'HumanPlayer' object has no attribute 'state_value'"
     ]
    }
   ],
   "source": [
    "experiment = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
